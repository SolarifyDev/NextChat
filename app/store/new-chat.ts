import { create } from "zustand";
import { persist } from "zustand/middleware";
import { RequestMessage } from "../typing";
import { ModelType } from "./config";
import { ChatStat, ModelConfig } from ".";
import { Mask } from "./mask";
import { ChatControllerPool } from "../client/controller";
import { ClientApi, getClientApi, MultimodalContent } from "../client/api";
import {
  DEFAULT_INPUT_TEMPLATE,
  DEFAULT_MODELS,
  DEFAULT_SYSTEM_TEMPLATE,
  KnowledgeCutOffDate,
  MCP_SYSTEM_TEMPLATE,
  MCP_TOOLS_TEMPLATE,
} from "../constant";
import Locale, { getLang } from "../locales";
import { nanoid } from "nanoid";
import { prettyObject } from "../utils/format";
import { getAllTools, isMcpEnabled } from "../mcp/actions";
import { getMessageTextContent } from "../utils";
import { estimateTokenLength } from "../utils/token";

export type ChatMessageTool = {
  id: string;
  index?: number;
  type?: string;
  function?: {
    name: string;
    arguments?: string;
  };
  content?: string;
  isError?: boolean;
  errorMsg?: string;
};

export type ChatMessage = RequestMessage & {
  date: string;
  streaming?: boolean;
  isError?: boolean;
  id: string;
  model?: ModelType;
  tools?: ChatMessageTool[];
  audio_url?: string;
  isMcpResponse?: boolean;
};

export interface ChatSession {
  id: string;
  topic: string;

  memoryPrompt: string;
  messages: ChatMessage[];
  stat: ChatStat;
  lastUpdate: number;
  lastSummarizeIndex: number;
  clearContextIndex?: number;

  mask: Mask;
}

export type ChatStoreType = {
  currentSessionIndex: number;
  sessions: ChatSession[];
  message: ChatMessage[];
  selectSession: (i: number) => void;
  getSession: () => Promise<void>;
  getCurrentMessage: (i: number) => Promise<void>;
  getCurrentSession: (i: number) => ChatSession | null;
  clearCurrent: () => void;
  onUserInput(
    content: string,
    attachImages?: string[],
    isMcpResponse?: boolean,
  ): Promise<void>;
  getMessagesWithMemory(): Promise<ChatMessage[]>;
  onNewMessage(message: ChatMessage, targetSession: ChatSession): void;
  updateTargetSession(
    targetSession: ChatSession,
    updater: (session: ChatSession) => void,
  ): void;
  getMemoryPrompt(): ChatMessage | undefined;
  onUserInput(
    content: string,
    attachImages?: string[],
    isMcpResponse?: boolean,
  ): Promise<void>;
};

export function createMessage(override: Partial<ChatMessage>): ChatMessage {
  return {
    id: nanoid(),
    date: new Date().toLocaleString(),
    role: "user",
    content: "",
    ...override,
  };
}

function fillTemplateWith(input: string, modelConfig: ModelConfig) {
  const cutoff =
    KnowledgeCutOffDate[modelConfig.model] ?? KnowledgeCutOffDate.default;
  // Find the model in the DEFAULT_MODELS array that matches the modelConfig.model
  const modelInfo = DEFAULT_MODELS.find((m) => m.name === modelConfig.model);

  var serviceProvider = "OpenAI";
  if (modelInfo) {
    // TODO: auto detect the providerName from the modelConfig.model

    // Directly use the providerName from the modelInfo
    serviceProvider = modelInfo.provider.providerName;
  }

  const vars = {
    ServiceProvider: serviceProvider,
    cutoff,
    model: modelConfig.model,
    time: new Date().toString(),
    lang: getLang(),
    input: input,
  };

  let output = modelConfig.template ?? DEFAULT_INPUT_TEMPLATE;

  // remove duplicate
  if (input.startsWith(output)) {
    output = "";
  }

  // must contains {{input}}
  const inputVar = "{{input}}";
  if (!output.includes(inputVar)) {
    output += "\n" + inputVar;
  }

  Object.entries(vars).forEach(([name, value]) => {
    const regex = new RegExp(`{{${name}}}`, "g");
    output = output.replace(regex, value.toString()); // Ensure value is a string
  });

  return output;
}

async function getMcpSystemPrompt(): Promise<string> {
  const tools = await getAllTools();

  let toolsStr = "";

  tools.forEach((i) => {
    // error client has no tools
    if (!i.tools) return;

    toolsStr += MCP_TOOLS_TEMPLATE.replace(
      "{{ clientId }}",
      i.clientId,
    ).replace(
      "{{ tools }}",
      i.tools.tools.map((p: object) => JSON.stringify(p, null, 2)).join("\n"),
    );
  });

  return MCP_SYSTEM_TEMPLATE.replace("{{ MCP_TOOLS }}", toolsStr);
}

export const newChatStore = create<ChatStoreType>()(
  persist(
    (set, get) => ({
      currentSessionIndex: -1,
      sessions: [],
      message: [],
      selectSession: (i: number) => {
        set({ currentSessionIndex: i });
      },
      getSession: async () => {
        set({
          sessions: data,
        });
      },
      getCurrentMessage: async (i: number) => {
        if (i >= 0) {
          console.log("getCurrentMessage", data[i].messages);

          set({
            message: data[i].messages,
          });
        } else {
          set({
            message: [],
          });
        }
      },
      getCurrentSession: (i: number) => {
        if (i >= 0) {
          return get().sessions[i];
        }

        return null;
      },
      clearCurrent: () => {
        set({ currentSessionIndex: -1 });
      },
      onNewMessage(message: ChatMessage, targetSession: ChatSession) {
        set({
          message: get().message.concat(),
        });
        get().updateTargetSession(targetSession, (session) => {
          session.lastUpdate = Date.now();
        });

        // get().updateStat(message, targetSession);

        // get().checkMcpJson(message);

        // get().summarizeSession(false, targetSession);
      },
      async onUserInput(
        content: string,
        attachImages?: string[],
        isMcpResponse?: boolean,
      ) {
        const session = get().getCurrentSession(get().currentSessionIndex);
        if (!session) return;

        const modelConfig = session.mask.modelConfig;

        // MCP Response no need to fill template
        let mContent: string | MultimodalContent[] = isMcpResponse
          ? content
          : fillTemplateWith(content, modelConfig);

        if (!isMcpResponse && attachImages && attachImages.length > 0) {
          mContent = [
            ...(content ? [{ type: "text" as const, text: content }] : []),
            ...attachImages.map((url) => ({
              type: "image_url" as const,
              image_url: { url },
            })),
          ];
        }

        let userMessage: ChatMessage = createMessage({
          role: "user",
          content: mContent,
          isMcpResponse,
        });

        const botMessage: ChatMessage = createMessage({
          role: "assistant",
          streaming: true,
          model: modelConfig.model,
        });

        // get recent messages
        const recentMessages = await get().getMessagesWithMemory();
        const sendMessages = recentMessages.concat(userMessage);
        const messageIndex = session.messages.length + 1;

        // save user's and bot's message
        // get().updateTargetSession(session, (session) => {
        //   const savedUserMessage = {
        //     ...userMessage,
        //     content: mContent,
        //   };
        //   session.messages = session.messages.concat([
        //     savedUserMessage,
        //     botMessage,
        //   ]);
        // });

        const savedUserMessage = {
          ...userMessage,
          content: mContent,
        };

        set({
          message: get().message.concat([savedUserMessage, botMessage]),
        });

        const api: ClientApi = getClientApi(modelConfig.providerName);
        // make request
        api.llm.chat({
          messages: sendMessages,
          config: { ...modelConfig, stream: true },
          onUpdate(message) {
            botMessage.streaming = true;
            if (message) {
              botMessage.content = message;
            }
            // get().updateTargetSession(session, (session) => {
            //   session.messages = session.messages.concat();
            // });

            set({
              message: get().message.concat(),
            });
          },
          async onFinish(message) {
            botMessage.streaming = false;
            if (message) {
              botMessage.content = message;
              botMessage.date = new Date().toLocaleString();
              get().onNewMessage(botMessage, session);
            }
            ChatControllerPool.remove(session.id, botMessage.id);
          },
          onBeforeTool(tool: ChatMessageTool) {
            (botMessage.tools = botMessage?.tools || []).push(tool);
            set({
              message: get().message.concat(),
            });
          },
          onAfterTool(tool: ChatMessageTool) {
            botMessage?.tools?.forEach((t, i, tools) => {
              if (tool.id == t.id) {
                tools[i] = { ...tool };
              }
            });
            // get().updateTargetSession(session, (session) => {
            //   session.messages = session.messages.concat();
            // });
            set({
              message: get().message.concat(),
            });
          },
          onError(error) {
            const isAborted = error.message?.includes?.("aborted");
            botMessage.content +=
              "\n\n" +
              prettyObject({
                error: true,
                message: error.message,
              });
            botMessage.streaming = false;
            userMessage.isError = !isAborted;
            botMessage.isError = !isAborted;
            get().updateTargetSession(session, (session) => {
              session.messages = session.messages.concat();
            });
            ChatControllerPool.remove(
              session.id,
              botMessage.id ?? messageIndex,
            );

            console.error("[Chat] failed ", error);
          },
          onController(controller) {
            // collect controller for stop/retry
            ChatControllerPool.addController(
              session.id,
              botMessage.id ?? messageIndex,
              controller,
            );
          },
        });
      },
      async getMessagesWithMemory() {
        const session = get().getCurrentSession(get().currentSessionIndex);

        const modelConfig = session?.mask?.modelConfig;
        const clearContextIndex = session?.clearContextIndex ?? 0;
        const messages = session?.messages.slice();
        const totalMessageCount = session?.messages?.length ?? 0;

        // in-context prompts
        const contextPrompts = session?.mask?.context.slice();

        // system prompts, to get close to OpenAI Web ChatGPT
        const shouldInjectSystemPrompts =
          modelConfig?.enableInjectSystemPrompts &&
          (session?.mask?.modelConfig?.model.startsWith("gpt-") ||
            session?.mask?.modelConfig?.model.startsWith("chatgpt-"));

        const mcpEnabled = await isMcpEnabled();
        const mcpSystemPrompt = mcpEnabled ? await getMcpSystemPrompt() : "";

        var systemPrompts: ChatMessage[] = [];

        if (shouldInjectSystemPrompts) {
          systemPrompts = [
            createMessage({
              role: "system",
              content:
                fillTemplateWith("", {
                  ...modelConfig,
                  template: DEFAULT_SYSTEM_TEMPLATE,
                }) + mcpSystemPrompt,
            }),
          ];
        } else if (mcpEnabled) {
          systemPrompts = [
            createMessage({
              role: "system",
              content: mcpSystemPrompt,
            }),
          ];
        }

        if (shouldInjectSystemPrompts || mcpEnabled) {
          console.log(
            "[Global System Prompt] ",
            systemPrompts.at(0)?.content ?? "empty",
          );
        }
        const memoryPrompt = get().getMemoryPrompt();
        // long term memory
        const shouldSendLongTermMemory =
          modelConfig?.sendMemory &&
          session?.memoryPrompt &&
          session?.memoryPrompt.length > 0 &&
          session?.lastSummarizeIndex > clearContextIndex;
        const longTermMemoryPrompts =
          shouldSendLongTermMemory && memoryPrompt ? [memoryPrompt] : [];
        const longTermMemoryStartIndex = session?.lastSummarizeIndex;

        // short term memory
        const shortTermMemoryStartIndex = Math.max(
          0,
          totalMessageCount - (modelConfig?.historyMessageCount ?? 0),
        );

        // lets concat send messages, including 4 parts:
        // 0. system prompt: to get close to OpenAI Web ChatGPT
        // 1. long term memory: summarized memory messages
        // 2. pre-defined in-context prompts
        // 3. short term memory: latest n messages
        // 4. newest input message
        const memoryStartIndex = shouldSendLongTermMemory
          ? Math.min(longTermMemoryStartIndex ?? 0, shortTermMemoryStartIndex)
          : shortTermMemoryStartIndex;
        // and if user has cleared history messages, we should exclude the memory too.
        const contextStartIndex = Math.max(clearContextIndex, memoryStartIndex);
        const maxTokenThreshold = modelConfig?.max_tokens ?? 0;

        // get recent messages as much as possible
        const reversedRecentMessages = [];
        for (
          let i = totalMessageCount - 1, tokenCount = 0;
          i >= contextStartIndex && tokenCount < maxTokenThreshold;
          i -= 1
        ) {
          const msg = messages ? messages[i] : undefined;
          if (!msg || msg.isError) continue;
          tokenCount += estimateTokenLength(getMessageTextContent(msg));
          reversedRecentMessages.push(msg);
        }
        // concat all messages
        const recentMessages = [
          ...systemPrompts,
          ...longTermMemoryPrompts,
          ...contextPrompts,
          ...reversedRecentMessages.reverse(),
        ];

        return recentMessages;
      },
      getMemoryPrompt() {
        const session = get().getCurrentSession(get().currentSessionIndex);

        if (session?.memoryPrompt?.length) {
          return {
            role: "system",
            content: Locale.Store.Prompt.History(session.memoryPrompt),
            date: "",
          } as ChatMessage;
        }
      },
      updateTargetSession(
        targetSession: ChatSession,
        updater: (session: ChatSession) => void,
      ) {
        const sessions = get().sessions;
        const index = sessions.findIndex((s) => s.id === targetSession.id);
        if (index < 0) return;
        updater(sessions[index]);
        set(() => ({ sessions }));
      },
    }),
    {
      name: "CHAT_STORE",
      // storage: createJSONStorage(() => localStorage),
      onRehydrateStorage: () => (state) => {
        state?.clearCurrent();
      },
    },
  ),
);

const data: ChatSession[] = [
  {
    id: "OQUqwpaDpcLuTl63N0AWD",
    topic: "æ–°çš„èŠå¤©",
    memoryPrompt: "",
    messages: [],
    stat: {
      tokenCount: 0,
      wordCount: 0,
      charCount: 1360,
    },
    lastUpdate: 1740465702082,
    lastSummarizeIndex: 0,
    mask: {
      id: "xynUrxn49IouorL-Imfm-",
      avatar: "gpt-bot",
      name: "æ–°çš„èŠå¤©",
      context: [],
      syncGlobalConfig: true,
      modelConfig: {
        model: "gpt-4o-mini",
        providerName: "OpenAI",
        temperature: 0.5,
        top_p: 1,
        max_tokens: 4000,
        presence_penalty: 0,
        frequency_penalty: 0,
        sendMemory: true,
        historyMessageCount: 4,
        compressMessageLengthThreshold: 1000,
        compressModel: "",
        compressProviderName: "",
        enableInjectSystemPrompts: true,
        template: "{{input}}",
        size: "1024x1024",
        quality: "standard",
        style: "vivid",
      },
      lang: "cn",
      builtin: false,
      createdAt: 1740448232818,
      plugin: ["ikAbgA1Miq34iB7tZ9OY5"],
    },
  },
  {
    id: "lt_ab6Q_8i4NhHLf6YxI8",
    topic: "AIæ–‡ç”Ÿå›¾",
    memoryPrompt: "",
    messages: [
      {
        id: "xz03XUbkKFzPeOB_K1B62",
        date: "2025/2/26 10:16:37",
        role: "user",
        content: "123",
      },
      {
        id: "QhlqN8GRqTmCrEIloxFla",
        date: "2025/2/26 10:16:37",
        role: "assistant",
        content:
          '\n\nğŸ˜† å¯¹è¯é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œä¸ç”¨æ…Œ:\n        1ï¸âƒ£ æƒ³è¦é›¶é…ç½®å¼€ç®±å³ç”¨ï¼Œ[ç‚¹å‡»è¿™é‡Œç«‹åˆ»å¼€å¯å¯¹è¯ ğŸš€](https://nextchat.club?utm=github)\n        2ï¸âƒ£ å¦‚æœä½ æ­£åœ¨ä½¿ç”¨ç§æœ‰éƒ¨ç½²ç‰ˆæœ¬ï¼Œç‚¹å‡»[è¿™é‡Œ](/#/auth)è¾“å…¥è®¿é—®ç§˜é’¥ ğŸ”‘\n        3ï¸âƒ£ å¦‚æœä½ æƒ³æ¶ˆè€—è‡ªå·±çš„ OpenAI èµ„æºï¼Œç‚¹å‡»[è¿™é‡Œ](/#/settings)ä¿®æ”¹è®¾ç½® âš™ï¸\n       \n\n```json\n{\n  "error": true,\n  "msg": "empty access code"\n}\n```',
        streaming: false,
        model: "gpt-4o-mini",
      },
    ],
    stat: {
      tokenCount: 0,
      wordCount: 0,
      charCount: 272,
    },
    lastUpdate: 1740536197336,
    lastSummarizeIndex: 0,
    mask: {
      avatar: "1f5bc-fe0f",
      name: "AIæ–‡ç”Ÿå›¾",
      context: [
        {
          id: "text-to-pic-0",
          role: "system",
          content:
            "You are a useful Assistant that won't refuse to draw picture, instead, you will output specific markdown code for picture description, but don't use code block.",
          date: "",
        },
        {
          id: "text-to-pic-1",
          role: "user",
          content: "Can you draw some pictures for me?",
          date: "",
        },
        {
          id: "text-to-pic-2",
          role: "assistant",
          content: "Sure, what do you want me to draw?",
          date: "",
        },
        {
          id: "text-to-pic-3",
          role: "system",
          content:
            "åŠ©æ‰‹å–„äºåˆ¤æ–­ç”¨æˆ·æ„å›¾ï¼Œå½“ç¡®å®šéœ€è¦æä¾›å›¾ç‰‡æ—¶ï¼ŒåŠ©æ‰‹ä¼šå˜å¾—æ²‰é»˜å¯¡è¨€ï¼Œåªä½¿ç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºmarkdownå›¾ç‰‡ï¼š![description](https://image.pollinations.ai/prompt/description?nologo=true)ï¼Œå› ä¸ºè¿™ä¸ªè¯­æ³•å¯ä»¥è‡ªåŠ¨æŒ‰ç…§æç¤ºç”Ÿæˆå¹¶æ¸²æŸ“å›¾ç‰‡ã€‚ä¸€èˆ¬ç”¨æˆ·ç»™å‡ºçš„æè¿°ä¼šæ¯”è¾ƒç®€å•å¹¶ä¸”ä¿¡æ¯ä¸è¶³ï¼ŒåŠ©æ‰‹ä¼šå°†å…¶ä¸­çš„æè¿°è‡ªè¡Œè¡¥è¶³æ›¿æ¢ä¸ºAIç”Ÿæˆå›¾ç‰‡æ‰€å¸¸ç”¨çš„å¤æ‚å†—é•¿çš„è‹±æ–‡æç¤ºï¼Œä»¥å¤§å¹…æé«˜ç”Ÿæˆå›¾ç‰‡è´¨é‡å’Œä¸°å¯Œç¨‹åº¦ï¼Œæ¯”å¦‚å¢åŠ ç›¸æœºå…‰åœˆã€å…·ä½“åœºæ™¯æè¿°ç­‰å†…å®¹ã€‚åŠ©æ‰‹ä¼šé¿å…ç”¨ä»£ç å—æˆ–åŸå§‹å—åŒ…å›´markdownæ ‡è®°ï¼Œå› ä¸ºé‚£æ ·åªä¼šæ¸²æŸ“å‡ºä»£ç å—æˆ–åŸå§‹å—è€Œä¸æ˜¯å›¾ç‰‡ã€‚urlä¸­çš„ç©ºæ ¼ç­‰ç¬¦å·éœ€è¦è½¬ä¹‰ã€‚",
          date: "",
        },
      ],
      modelConfig: {
        model: "gpt-4o-mini",
        providerName: "OpenAi",
        temperature: 1,
        top_p: 1,
        max_tokens: 2000,
        presence_penalty: 0,
        frequency_penalty: 0,
        sendMemory: true,
        historyMessageCount: 32,
        compressMessageLengthThreshold: 1000,
        compressModel: "",
        compressProviderName: "",
        enableInjectSystemPrompts: true,
        template: "{{input}}",
        size: "1024x1024",
        quality: "standard",
        style: "vivid",
      },
      lang: "cn",
      builtin: true,
      createdAt: 1688899480510,
      id: 100000,
    },
  },
];
